{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bda14fb0",
   "metadata": {},
   "source": [
    "**opencv-python**: Noraml verion with GUI <br>\n",
    "**opencv-python-headless**: no GUI, for backend <br>\n",
    "**opencv-contrib-python**: Extra modules and features <br>\n",
    "**opencv-contrib-python**: No GUI, for backend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b12be7b",
   "metadata": {},
   "source": [
    "**Read, Display and Write an Image using OpenCV**\n",
    "\n",
    "1. imread() -> READ AN image\n",
    "2. imshow() -> Display an image\n",
    "3. imwrite() -> Writing an Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e666f5d",
   "metadata": {},
   "source": [
    "**Reading an Image**\n",
    "For reading an image, use the imread()function in OpenCV. Here‚Äôs the syntax:\n",
    "```\n",
    "    imread(filename, flags)\n",
    "```\n",
    "It takes two arguments:\n",
    "\n",
    "The first argument is the image name, which requires a fully qualified pathname to the file.\n",
    "The second argument is an optional flag that lets you specify how the image should be represented. OpenCV offers several options for this flag, but those that are most common include:\n",
    "```\n",
    "    cv2.IMREAD_UNCHANGED  or -1\n",
    "    cv2.IMREAD_GRAYSCALE  or 0\n",
    "    cv2.IMREAD_COLOR  or 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc995b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "930dcd20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_grayscale = cv2.imread(\"img/aero1.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "cv2.imwrite(\"img/aero1_grayscale.jpg\", img_grayscale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07fabd7",
   "metadata": {},
   "source": [
    "The default value for flags is 1, which will read in the image as a Colored image.  When you want to read in an image in a particular format,  just specify the appropriate flag.\n",
    "\n",
    "It‚Äôs also important to note at this point that OpenCV reads color images in BGR format, whereas most other computer vision libraries use the RGB channel format order. So, when using OpenCV with other toolkits, don‚Äôt forget to swap the blue and red color channels, as you switch from one library to another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8088478",
   "metadata": {},
   "source": [
    "**Displaying an Image**\n",
    "\n",
    "In OpenCV, you display an image using the imshow() function. Here‚Äôs the syntax:\n",
    "```\n",
    "    imshow(window_name, image)\n",
    "```\n",
    "This function also takes two arguments:\n",
    "\n",
    "1. The first argument is the window name that will be displayed on the window.\n",
    "2. The second argument is the image that you want to display. \n",
    "\n",
    "To display multiple images at once, specify a new window name for every image you want to display. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1da14a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"color image\", cv2.imread(\"img/aero1.jpg\"))\n",
    "cv2.imshow(\"grayscale image\", cv2.imread(\"img/aero1_grayscale.jpg\"))\n",
    "cv2.imshow(\"unchanged image\", cv2.imread(\"img/aero1.jpg\"))\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044746c9",
   "metadata": {},
   "source": [
    "**Writing an Image**\n",
    "\n",
    "let‚Äôs discuss how to write/save an image into the file directory, using the imwrite() function. Check out its syntax:\n",
    "```\n",
    "    imwrite(filename, image).\n",
    "```\n",
    "1. The first argument is the filename, which must include the filename extension (for example .png, .jpg etc). OpenCV uses this filename extension to specify the format of the file. \n",
    "2. The second argument is the image you want to save. The function returns True if the image is saved successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f8593e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite(\"img/aero1_grayscale.jpg\", img_grayscale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b348f8f",
   "metadata": {},
   "source": [
    "### Reading and Writing Videos using OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9721710d",
   "metadata": {},
   "source": [
    "Reading and writing videos in OpenCV is very similar to reading and writing images. A video is nothing but a series of images that are often referred to as frames. So, all you need to do is loop over all the frames in a video sequence, and then process one frame at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2d3c50",
   "metadata": {},
   "source": [
    "1. Reading videos\n",
    "```\n",
    "    From a file\n",
    "    From Image-sequence\n",
    "    From a webcam\n",
    "```\n",
    "2. Writing videos\n",
    "3. Errors That One Might Face when Reading or Writing Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcd98b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_capture = cv2.VideoCapture(\"videos/Cars.mp4\")\n",
    "\n",
    "if (vid_capture.isOpened() == False):\n",
    "    print(\"Error opening video stream or file\")\n",
    "else:\n",
    "    fps = vid_capture.get(5)\n",
    "    print(\"Frames per second : {0}\".format(fps))\n",
    "\n",
    "    frame_count = vid_capture.get(7)\n",
    "    print('Frame count : ', frame_count)\n",
    "\n",
    "while (vid_capture.isOpened()):\n",
    "    ret, frame = vid_capture.read()\n",
    "    if ret == True:\n",
    "        cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "        if cv2.waitKey(25) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Release the Video capture object\n",
    "vid_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0075e1a3",
   "metadata": {},
   "source": [
    "Now that we have a video capture object we can use the isOpened() method to confirm the video file was opened successfully. The isOpened() method returns a boolean that indicates whether or not the video stream is valid. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0280d886",
   "metadata": {},
   "source": [
    "**Reading Video From a File**\n",
    "The next code block below uses the VideoCapture() class to create a VideoCapture object, which we will then use to read the video file. The syntax for using this class is shown below: \n",
    "```\n",
    "    VideoCapture(path, apiPreference)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c531575",
   "metadata": {},
   "source": [
    "**Reading an Image Sequence**\n",
    "Processing image frames from an image sequence is very similar to processing frames from a video stream. Just specify the image files which are being read. \n",
    "\n",
    "In the example below, \n",
    "\n",
    "You continue using a video-capture object\n",
    "But instead of specifying a video file, you simply specify an image sequence\n",
    "Using the notation shown below (Cars%04d.jpg), where %04d indicates a four-digit sequence-naming convention (e.g. Cars0001.jpg, Cars0002.jpg, Cars0003.jpg, etc).  \n",
    "If you had specified ‚ÄúRace_Cars_%02d.jpg‚Äù then you would be looking for files of the form: \n",
    "(Race_Cars_01.jpg, Race_Cars_02.jpg, Race_Cars_03.jpg, etc‚Ä¶). \n",
    "```python\n",
    "\t\n",
    "    vid_capture = cv2.VideoCapture('Resources/Image_sequence/Cars%04d.jpg')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11064c08",
   "metadata": {},
   "source": [
    "**Reading Video from a Webcam**\n",
    "\n",
    "Reading a video stream from a web camera is also very similar to the examples discussed above. How‚Äôs that possible? It‚Äôs all thanks to the flexibility of the video capture class in OpenCV, which has several overloaded functions for convenience that accept different input arguments. Rather than specifying a source location for a video file or an image sequence, you simply need to give a video capture device index, as shown below. \n",
    "\n",
    "* If your system has a built-in webcam, then the device index for the camera will be ‚Äò0‚Äô. \n",
    "* If you have more than one camera connected to your system, then the device index associated with each additional camera is incremented (e.g. 1, 2, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69382933",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_capture = cv2.VideoCapture(0, cv2.CAP_DSHOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aa56c9",
   "metadata": {},
   "source": [
    "Writing Videos\n",
    "Let‚Äôs now take a look at how to write videos. Just like video reading, we can write videos originating from any source (a video file, an image sequence, or a webcam). To write a video file: \n",
    "\n",
    "Retrieve the image frame height and width, using the get() method.\n",
    "Initialize a video capture object (as discussed in the previous sections), to read the video stream into memory, using any of the sources previously described.\n",
    "Create a video writer object.\n",
    "Use the video writer object to save the video stream to disk. \n",
    "\n",
    "```python\n",
    "# Obtain frame size information using get() method\n",
    "frame_width = int(vid_capture.get(3))\n",
    "frame_height = int(vid_capture.get(4))\n",
    "frame_size = (frame_width,frame_height)\n",
    "fps = 20\n",
    "\n",
    "#Here‚Äôs the syntax for VideoWriter():\n",
    "\n",
    "VideoWriter(filename, apiPreference, fourcc, fps, frameSize[, isColor])\n",
    "```\n",
    "```\n",
    "The VideoWriter() class takes the following arguments: \n",
    "\n",
    "    filename: pathname for the output video file\n",
    "    apiPreference:  API backends identifier\n",
    "    fourcc: 4-character code of codec, used to compress the frames (fourcc)\n",
    "    fps: Frame rate of the created video stream\n",
    "    frame_size: Size of the video frames\n",
    "    isColor: If not zero, the encoder will expect and encode color frames. Else it will work with grayscale frames (the flag is currently supported on Windows only).\n",
    "\n",
    "The following code creates the video writer object, output from the VideoWriter() class. A special convenience function is used to retrieve the four-character codec, required as the second argument to the video writer object, cv2.\n",
    "\n",
    "    VideoWriter_fourcc('M', 'J', 'P', 'G') in Python.\n",
    "    VideoWriter::fourcc('M', 'J', 'P', 'G')in C++.\n",
    "The video codec specifies how the video stream is compressed. It converts uncompressed video to a compressed format or vice versa. To create AVI or MP4 formats, use the following fourcc specifications:\n",
    "\n",
    "AVI: cv2.VideoWriter_fourcc('M','J','P','G')\n",
    "\n",
    "MP4: cv2.VideoWriter_fourcc(*'XVID')\n",
    "```\n",
    "```python\n",
    "output = cv2.VideoWriter('Resources/output_video_from_file.avi', cv2.VideoWriter_fourcc('M','J','P','G'), 20, frame_size)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd43aa6",
   "metadata": {},
   "source": [
    "### Image Resizing with OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05271eea",
   "metadata": {},
   "source": [
    "Image resizing means changing the width and/or height of an image. This can be done to shrink (downscale) or enlarge (upscale) an image\n",
    "```\n",
    "    Image resizing with a custom Width and Height\n",
    "    Resizing an image with a Scaling factor\n",
    "    Image resizing with different Interpolation methods\n",
    "```\n",
    "  **Resizing by Specifying Width and Height**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81d0556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the image using imread function\n",
    "image = cv2.imread('image.jpg')\n",
    "cv2.imshow('Original Image', image)\n",
    " \n",
    "# let's downscale the image using new  width and height\n",
    "down_width = 300\n",
    "down_height = 200\n",
    "down_points = (down_width, down_height)\n",
    "resized_down = cv2.resize(image, down_points, interpolation= cv2.INTER_LINEAR)\n",
    " \n",
    "# let's upscale the image using new  width and height\n",
    "up_width = 600\n",
    "up_height = 400\n",
    "up_points = (up_width, up_height)\n",
    "resized_up = cv2.resize(image, up_points, interpolation= cv2.INTER_LINEAR)\n",
    " \n",
    "# Display images\n",
    "cv2.imshow('Resized Down by defining height and width', resized_down)\n",
    "cv2.waitKey()\n",
    "cv2.imshow('Resized Up image by defining height and width', resized_up)\n",
    "cv2.waitKey()\n",
    " \n",
    "#press any key to close the windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682342c0",
   "metadata": {},
   "source": [
    "```\n",
    "Before you start resizing the image, know its original size. To obtain the size of an image:\n",
    "\n",
    "use the shape method in Python\n",
    "rows and cols in C++ \n",
    "image.shape in Python returns three values: Height, width and number of channels.\n",
    "\n",
    "In C++:\n",
    "\n",
    "image.rows gives you the height\n",
    "image.columns gives you the width of the image \n",
    "The above results can also be obtained, using the size() function. \n",
    "\n",
    "image.size().width returns the width\n",
    "image.size().height returns the height\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf33a1",
   "metadata": {},
   "source": [
    "```python\n",
    "# Get original height and width\n",
    "h,w,c = image.shape\n",
    "print(\"Original Height and Width:\", h,\"x\", w)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f33f53d",
   "metadata": {},
   "source": [
    "**Resize Function Syntax**\n",
    "\n",
    "Let‚Äôs begin by taking a look at the OpenCV resize() function syntax. Notice that only two  input arguments are required: \n",
    "```\n",
    "The source image.\n",
    "The desired size of the resized image, dsize.\n",
    "We will discuss the various input argument options in the sections below.\n",
    "\n",
    "    resize(src, dsize[, dst[, fx[, fy[, interpolation]]]])\n",
    "\n",
    "    src: It is the required input image, it could be a string with the path of the input image (eg: ‚Äòtest_image.png‚Äô).\n",
    "    dsize: It is the desired size of the output image, it can be a new height and width.\n",
    "    fx: Scale factor along the horizontal axis.\n",
    "    fy: Scale factor along the vertical axis.\n",
    "    interpolation: It gives us the option of different methods of resizing the image.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859724c8",
   "metadata": {},
   "source": [
    "**Resizing With a Scaling Factor**\n",
    "now we resize the image with a scaling factor. But before going further, you need to understand what exactly is a scaling factor. \n",
    "\n",
    "Scaling Factor or Scale Factor is usually a number that scales or multiplies some quantity, in our case the width and height of the image. It helps keep the aspect ratio intact and preserves the display quality. So the image does not appear distorted, while you are upscaling or downscaling it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f3e059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling Up the image 1.2 times by specifying both scaling factors\n",
    "scale_up_x = 1.2\n",
    "scale_up_y = 1.2\n",
    "# Scaling Down the image 0.6 times specifying a single scale factor.\n",
    "scale_down = 0.6\n",
    " \n",
    "scaled_f_down = cv2.resize(image, None, fx= scale_down, fy= scale_down, interpolation= cv2.INTER_LINEAR)\n",
    "scaled_f_up = cv2.resize(image, None, fx= scale_up_x, fy= scale_up_y, interpolation= cv2.INTER_LINEAR)\n",
    "\n",
    "# Display images and press any key to check next image\n",
    "cv2.imshow('Resized Down by defining scaling factor', scaled_f_down)\n",
    "cv2.waitKey()\n",
    "cv2.imshow('Resized Up image by defining scaling factor', scaled_f_up)\n",
    "cv2.waitKey()\n",
    "\n",
    "# We define new scaling factors along the horizontal and vertical axis. \n",
    "# Defining the scaling factors removes the need to have new points for width and height. Hence, we keep dsize as None. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79c12a3",
   "metadata": {},
   "source": [
    "**Resizing With Different Interpolation Methods**\n",
    "\n",
    "Different interpolation methods are used for different resizing purposes.\n",
    "```\n",
    "    INTER_AREA: INTER_AREA uses pixel area relation for resampling. This is best suited for reducing the size of an image (shrinking). When used for zooming into the image, it uses the INTER_NEAREST method.\n",
    "    INTER_CUBIC: This uses bicubic interpolation for resizing the image. While resizing and interpolating new pixels, this method acts on the 4√ó4 neighboring pixels of the image. It then takes the weights average of the 16 pixels to create the new interpolated pixel.\n",
    "    INTER_LINEAR: This method is somewhat similar to the INTER_CUBIC interpolation. But unlike INTER_CUBIC, this uses 2√ó2 neighboring pixels to get the weighted average for the interpolated pixel.\n",
    "    INTER_NEAREST: The INTER_NEAREST method uses the nearest neighbor concept for interpolation. This is one of the simplest methods, using only one neighboring pixel from the image for interpolation.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2a7b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling Down the image 0.6 times using different Interpolation Method\n",
    "res_inter_nearest = cv2.resize(image, None, fx= scale_down, fy= scale_down, interpolation= cv2.INTER_NEAREST)\n",
    "res_inter_linear = cv2.resize(image, None, fx= scale_down, fy= scale_down, interpolation= cv2.INTER_LINEAR)\n",
    "res_inter_area = cv2.resize(image, None, fx= scale_down, fy= scale_down, interpolation= cv2.INTER_AREA)\n",
    "\n",
    "\t\n",
    "# Concatenate images in horizontal axis for comparison\n",
    "vertical= np.concatenate((res_inter_nearest, res_inter_linear, res_inter_area), axis = 0)\n",
    "# Display the image Press any key to continue\n",
    "cv2.imshow('Inter Nearest :: Inter Linear :: Inter Area', vertical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c98b69",
   "metadata": {},
   "source": [
    "### Cropping an Image using OpenCV\n",
    "\n",
    "There is no specific function for cropping using OpenCV, NumPy array slicing is what does the job. Every image that is read in, gets stored in a 2D array (for each color channel). Simply specify the height and width (in pixels) of the area to be cropped. And it‚Äôs done!\n",
    "\n",
    "```\n",
    "    Cropping using OpenCV\n",
    "    Diving an Image into Small Patches\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d640a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import cv2\n",
    "import numpy as np\n",
    " \n",
    "img = cv2.imread('test.jpg')\n",
    "print(img.shape) # Print image shape\n",
    "cv2.imshow(\"original\", img)\n",
    " \n",
    "# Cropping an image\n",
    "# cropped = img[start_row:end_row, start_col:end_col]\n",
    "cropped_image = img[80:280, 150:330]\n",
    " \n",
    "# Display cropped image\n",
    "cv2.imshow(\"cropped\", cropped_image)\n",
    " \n",
    "# Save the cropped image\n",
    "cv2.imwrite(\"Cropped Image.jpg\", cropped_image)\n",
    " \n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a40b7bb",
   "metadata": {},
   "source": [
    "**Dividing an Image Into Small Patches Using Cropping**\n",
    "\n",
    "One practical application of cropping in OpenCV can be to divide an image into smaller patches. Use loops to crop out a fragment from the image. Start by getting the height and width of the required patch from the shape of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "543c5b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "img =  cv2.imread(\"img/apple.jpg\")\n",
    "image_copy = img.copy()\n",
    "imgheight=img.shape[0]\n",
    "imgwidth=img.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "439e56ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 76\n",
    "N = 104\n",
    "x1 = 0\n",
    "y1 = 0\n",
    " \n",
    "for y in range(0, imgheight, M):\n",
    "    for x in range(0, imgwidth, N):\n",
    "        if (imgheight - y) < M or (imgwidth - x) < N:\n",
    "            break\n",
    "             \n",
    "        y1 = y + M\n",
    "        x1 = x + N\n",
    " \n",
    "        # check whether the patch width or height exceeds the image width or height\n",
    "        if x1 >= imgwidth and y1 >= imgheight:\n",
    "            x1 = imgwidth - 1\n",
    "            y1 = imgheight - 1\n",
    "            #Crop into patches of size MxN\n",
    "            tiles = image_copy[y:y+M, x:x+N]\n",
    "            #Save each patch into file directory\n",
    "            cv2.imwrite('saved_patches/'+'tile'+str(x)+'_'+str(y)+'.jpg', tiles)\n",
    "            cv2.rectangle(img, (x, y), (x1, y1), (0, 255, 0), 1)\n",
    "        elif y1 >= imgheight: # when patch height exceeds the image height\n",
    "            y1 = imgheight - 1\n",
    "            #Crop into patches of size MxN\n",
    "            tiles = image_copy[y:y+M, x:x+N]\n",
    "            #Save each patch into file directory\n",
    "            cv2.imwrite('saved_patches/'+'tile'+str(x)+'_'+str(y)+'.jpg', tiles)\n",
    "            cv2.rectangle(img, (x, y), (x1, y1), (0, 255, 0), 1)\n",
    "        elif x1 >= imgwidth: # when patch width exceeds the image width\n",
    "            x1 = imgwidth - 1\n",
    "            #Crop into patches of size MxN\n",
    "            tiles = image_copy[y:y+M, x:x+N]\n",
    "            #Save each patch into file directory\n",
    "            cv2.imwrite('saved_patches/'+'tile'+str(x)+'_'+str(y)+'.jpg', tiles)\n",
    "            cv2.rectangle(img, (x, y), (x1, y1), (0, 255, 0), 1)\n",
    "        else:\n",
    "            #Crop into patches of size MxN\n",
    "            tiles = image_copy[y:y+M, x:x+N]\n",
    "            #Save each patch into file directory\n",
    "            cv2.imwrite('saved_patches/'+'tile'+str(x)+'_'+str(y)+'.jpg', tiles)\n",
    "            cv2.rectangle(img, (x, y), (x1, y1), (0, 255, 0), 1)\n",
    "\n",
    "#Save full image into file directory\n",
    "cv2.imshow(\"Patched Image\",img)\n",
    "cv2.imwrite(\"patched.jpg\",img)\n",
    "  \n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1eea37",
   "metadata": {},
   "source": [
    "### Image Translation and Rotation Using OpenCV\n",
    "\n",
    "**Image Rotation using OpenCV**\n",
    "```\n",
    "OpenCV provides the **getRotationMatrix2D()** function to create the above transformation matrix.\n",
    "\n",
    "The following is the syntax for creating the 2D rotation matrix:\n",
    "\n",
    "    getRotationMatrix2D(center, angle, scale)\n",
    "\n",
    "The getRotationMatrix2D() function takes the following arguments:\n",
    "\n",
    "center: the center of rotation for the input image\n",
    "angle: the angle of rotation in degrees\n",
    "scale: an isotropic scale factor that scales the image up or down according to the value provided\n",
    "If the angle is positive, the image gets rotated in the counter-clockwise direction. If you want to rotate the image clockwise by the same amount, then the angle needs to be negative.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b10584a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    " \n",
    "# Reading the image\n",
    "image = cv2.imread('img/aero3.jpg')\n",
    " \n",
    "# dividing height and width by 2 to get the center of the image\n",
    "height, width = image.shape[:2]\n",
    "# get the center coordinates of the image to create the 2D rotation matrix\n",
    "center = (width/2, height/2)\n",
    " \n",
    "# using cv2.getRotationMatrix2D() to get the rotation matrix\n",
    "rotate_matrix = cv2.getRotationMatrix2D(center=center, angle=45, scale=1)\n",
    " \n",
    "# rotate the image using cv2.warpAffine\n",
    "rotated_image = cv2.warpAffine(src=image, M=rotate_matrix, dsize=(width, height))\n",
    " \n",
    "cv2.imshow('Original image', image)\n",
    "cv2.imshow('Rotated image', rotated_image)\n",
    "# wait indefinitely, press any key on keyboard to exit\n",
    "cv2.waitKey(0)\n",
    "# save the rotated image to disk\n",
    "cv2.imwrite('rotated_image.jpg', rotated_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f703c3",
   "metadata": {},
   "source": [
    "**Rotation is a three-step operation:**\n",
    "\n",
    "First, you need to get the center of rotation. This typically is the center of the image you are trying to rotate.\n",
    "Next, create the 2D-rotation matrix. OpenCV provides the getRotationMatrix2D() function that we discussed above. \n",
    "Finally, apply the affine transformation to the image, using the rotation matrix you created in the previous step. The warpAffine() function in OpenCV does the job.\n",
    "\n",
    "The warpAffine() function applies an affine transformation to the image. After applying affine transformation, all the parallel lines in the original image will remain parallel in the output image as well.\n",
    "\n",
    "The complete syntax for warpAffine() is given below:\n",
    "\n",
    "warpAffine(src, M, dsize[, dst[, flags[, borderMode[, borderValue]]]])\n",
    "\n",
    "The following are the arguments of the function:\n",
    "```\n",
    "    src: the source mage\n",
    "    M: the transformation matrix\n",
    "    dsize: size of the output image\n",
    "    dst: the output image\n",
    "    flags: combination of interpolation methods such as INTER_LINEAR or INTER_NEAREST\n",
    "    borderMode: the pixel extrapolation method\n",
    "    borderValue: the value to be used in case of a constant border, has a default value of 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8d2236",
   "metadata": {},
   "source": [
    "**Image Translation using OpenCV**\n",
    "\n",
    "In computer vision, image translation means shifting it by a specified number of pixels, along the x and y axes. Let the pixels by which the image needs to shifted be tx and ty.\n",
    "\n",
    "Now, there are a few points you should keep in mind while shifting the image by tx and ty values.\n",
    "\n",
    "Providing positive values for tx will shift the image to the right, and negative values will shift the image to the left.\n",
    "Similarly, positive values of ty will shift the image down, while negative values will shift the image up.\n",
    "Follow these steps to translate an image using OpenCV:\n",
    "\n",
    "First, read the image and obtain its width and height.\n",
    "Next, like you did for rotation, create a transformation matrix, which is a 2D array. This matrix contains the information needed to shift the image, along the x and y axes.\n",
    "\n",
    "Again, as in rotation, use the warpAffine() function, in this final step, to apply the affine transformation.\n",
    "Go through this code and see for yourself how simple it is:."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b9b695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    " \n",
    "# read the image\n",
    "image = cv2.imread('image.jpg')\n",
    "# get the width and height of the image\n",
    "height, width = image.shape[:2]\n",
    "\n",
    "# get tx and ty values for translation\n",
    "# you can specify any value of your choice\n",
    "tx, ty = width / 4, height / 4\n",
    " \n",
    "# create the translation matrix using tx and ty, it is a NumPy array\n",
    "translation_matrix = np.array([\n",
    "    [1, 0, tx],\n",
    "    [0, 1, ty]\n",
    "], dtype=np.float32)\n",
    "\n",
    "# apply the translation to the image\n",
    "translated_image = cv2.warpAffine(src=image, M=translation_matrix, dsize=(width, height))\n",
    "\n",
    "# display the original and the Translated images\n",
    "cv2.imshow('Translated image', translated_image)\n",
    "cv2.imshow('Original image', image)\n",
    "cv2.waitKey(0)\n",
    "# save the translated image to disk\n",
    "cv2.imwrite('translated_image.jpg', translated_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7e1a62",
   "metadata": {},
   "source": [
    "### Annotating Images Using OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547d9e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import cv2\n",
    "# Read Images\n",
    "img = cv2.imread('sample.jpg')\n",
    "# Display Image\n",
    "cv2.imshow('Original Image',img)\n",
    "cv2.waitKey(0)\n",
    "# Print error message if image is null\n",
    "if img is None:\n",
    "    print('Could not read image')\n",
    "# Draw line on image\n",
    "imageLine = img.copy()\n",
    "\n",
    "# Draw the image from point A to B\n",
    "pointA = (200,80)\n",
    "pointB = (450,80)\n",
    "cv2.line(imageLine, pointA, pointB, (255, 255, 0), thickness=3, lineType=cv2.LINE_AA)\n",
    "cv2.imshow('Image Line', imageLine)\n",
    "cv2.waitKey(0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd0ec07",
   "metadata": {},
   "source": [
    "**Draw a Line**\n",
    "```\n",
    "In this first example, let‚Äôs annotate the image with a color line, using the line() function in OpenCV. Before calling the line() function, create a copy of the original image by using:\n",
    "\n",
    " the copy() function in Python\n",
    " the clone() function in C++ \n",
    "A copy will ensure that any changes you make to the image will not affect the original image. In C++, you first create a matrix for the copy of the original image.\n",
    "\n",
    "Here‚Äôs the syntax for the line() function:\n",
    "\n",
    "line(image, start_point, end_point, color, thickness)\n",
    "\n",
    "The first argument is the image. \n",
    "The next two arguments are the starting point and ending point for the line.  \n",
    "Draw a line from point A(x1, y1) to point B(x2, y2), where A and B represent any two points in the image. Look at the top left corner of the image, you‚Äôll find there the origin of the xy coordinate system.\n",
    "\n",
    "The x-axis represents the horizontal direction or the columns of the image.\n",
    "The y-axis represents the vertical direction or the rows of the image.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef2ad48",
   "metadata": {},
   "source": [
    "``` basic line\n",
    "1. Line\n",
    "cv2.line(img, pt1, pt2, color, thickness, lineType, shift)\n",
    "2. Arrowed Line\n",
    "cv2.arrowedLine(img, pt1, pt2, color, thickness, lineType, shift, tipLength)\n",
    "\n",
    "3. Rectangle\n",
    "cv2.rectangle(img, pt1, pt2, color, thickness, lineType, shift)\n",
    "\n",
    "4. Circle\n",
    "cv2.circle(img, center, radius, color, thickness, lineType, shift)\n",
    "\n",
    "5. Ellipse\n",
    "cv2.ellipse(img, center, axes, angle, startAngle, endAngle, color, thickness)\n",
    "6. Draw Contours\n",
    "cv2.drawContours(img, contours, contourIdx, color, thickness)\n",
    "\n",
    "üîπ Polygon\n",
    "7. Polylines\n",
    "cv2.polylines(img, [pts], isClosed, color, thickness)\n",
    "8. Fill Polygon\n",
    "\n",
    "cv2.fillPoly(img, [pts], color)\n",
    "\n",
    "üîπ Text Functions\n",
    "9. Put Text\n",
    "cv2.putText(img, text, org, font, fontScale, color, thickness, lineType)\n",
    "\n",
    "10. Get Text Size (IMPORTANT ‚Äì often missed)\n",
    "cv2.getTextSize(text, font, fontScale, thickness)\n",
    "üîπ Marker Function (Often Missed)\n",
    "11. Draw Marker\n",
    "cv2.drawMarker(img, position, color, markerType, markerSize, thickness)\n",
    "\n",
    "Marker types:\n",
    "\n",
    "    cv2.MARKER_CROSS\n",
    "    cv2.MARKER_STAR\n",
    "    cv2.MARKER_TILTED_CROSS\n",
    "    cv2.MARKER_DIAMOND\n",
    "    cv2.MARKER_SQUARE\n",
    "    cv2.MARKER_TRIANGLE_UP\n",
    "    cv2.MARKER_TRIANGLE_DOWN\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5be9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = np.ones((600, 900, 3), dtype=np.uint8) * 255\n",
    "\n",
    "# Line\n",
    "cv2.line(img, (50, 50), (300, 50), (255, 0, 0), 3)\n",
    "\n",
    "# Arrowed Line\n",
    "cv2.arrowedLine(img, (50, 90), (300, 90), (0, 0, 255), 3)\n",
    "\n",
    "# Rectangle\n",
    "cv2.rectangle(img, (50, 130), (300, 230), (0, 255, 0), 3)\n",
    "\n",
    "# Filled Rectangle\n",
    "cv2.rectangle(img, (350, 130), (600, 230), (0, 255, 255), -1)\n",
    "\n",
    "# Circle\n",
    "cv2.circle(img, (150, 350), 60, (255, 0, 255), 3)\n",
    "\n",
    "# Ellipse\n",
    "cv2.ellipse(img, (400, 350), (100, 50), 45, 0, 360, (0, 128, 255), 3)\n",
    "\n",
    "# Polygon\n",
    "pts = np.array([[650, 150], [850, 150], [800, 250], [700, 250]], np.int32)\n",
    "pts = pts.reshape((-1, 1, 2))\n",
    "cv2.polylines(img, [pts], True, (128, 0, 128), 3)\n",
    "\n",
    "# Filled Polygon\n",
    "pts2 = np.array([[650, 300], [850, 300], [800, 400], [700, 400]], np.int32)\n",
    "pts2 = pts2.reshape((-1, 1, 2))\n",
    "cv2.fillPoly(img, [pts2], (200, 200, 0))\n",
    "\n",
    "# Draw Marker\n",
    "cv2.drawMarker(img, (100, 500), (0, 0, 0),\n",
    "               markerType=cv2.MARKER_STAR,\n",
    "               markerSize=30,\n",
    "               thickness=2)\n",
    "\n",
    " # types of markers available in OpenCV are:\n",
    "    # cv2.MARKER_CROSS\n",
    "    # cv2.MARKER_STAR\n",
    "    # cv2.MARKER_TILTED_CROSS\n",
    "    # cv2.MARKER_DIAMOND\n",
    "    # cv2.MARKER_SQUARE\n",
    "    # cv2.MARKER_TRIANGLE_UP\n",
    "    # cv2.MARKER_TRIANGLE_DOWN\n",
    "\n",
    "# Draw Contours\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "ret, thresh = cv2.threshold(gray, 200, 255, cv2.THRESH_BINARY_INV)\n",
    "contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "cv2.drawContours(img, contours, -1, (0, 0, 255), 1)\n",
    "\n",
    "# Text with background box (using getTextSize)\n",
    "text = \"Complete OpenCV Annotation\"\n",
    "font = cv2.FONT_HERSHEY_COMPLEX\n",
    "font_scale = 1\n",
    "thickness = 2\n",
    "\n",
    "(text_w, text_h), baseline = cv2.getTextSize(text, font, font_scale, thickness)\n",
    "\n",
    "x, y = 200, 580\n",
    "\n",
    "# Background rectangle for text\n",
    "cv2.rectangle(img,\n",
    "              (x, y - text_h - baseline),\n",
    "              (x + text_w, y + baseline),\n",
    "              (220, 220, 220),\n",
    "              -1)\n",
    "\n",
    "# Put text\n",
    "cv2.putText(img, text, (x, y),\n",
    "            font, font_scale,\n",
    "            (0, 0, 0),\n",
    "            thickness)\n",
    "\n",
    "cv2.imshow(\"All Annotation Functions\", img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116a4c6f",
   "metadata": {},
   "source": [
    "### Color spaces in OpenCV \n",
    "\n",
    "1. The RGB Color Space\n",
    "The RGB colorspace has the following properties\n",
    "\n",
    "It is an additive colorspace where colors are obtained by a linear combination of Red, Green, and Blue values.\n",
    "The three channels are correlated by the amount of light hitting the surface.\n",
    "\n",
    "2.The LAB Color-Space\n",
    "The Lab color space has three components.\n",
    "\n",
    "    L ‚Äì Lightness ( Intensity ).\n",
    "    a ‚Äì color component ranging from Green to Magenta.\n",
    "    b ‚Äì color component ranging from Blue to Yellow.\n",
    "The Lab color space is quite different from the RGB color space. In RGB color space the color information is separated into three channels but the same three channels also encode brightness information. On the other hand, in Lab color space, the L channel is independent of color information and encodes brightness only. The other two channels encode color.\n",
    "\n",
    "3. The YCrCb Color-Space\n",
    "The YCrCb color space is derived from the RGB color space and has the following three compoenents.\n",
    "\n",
    "    Y ‚Äì Luminance or Luma component obtained from RGB after gamma correction.\n",
    "Cr = R ‚Äì Y ( how far is the red component from Luma ).\n",
    "Cb = B ‚Äì Y ( how far is the blue component from Luma ).\n",
    "\n",
    "4. The HSV Color Space\n",
    "The HSV color space has the following three components\n",
    "\n",
    "    H ‚Äì Hue ( Dominant Wavelength ).\n",
    "    S ‚Äì Saturation ( Purity / shades of the color ).\n",
    "    V ‚Äì Value ( Intensity ).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e616e708",
   "metadata": {},
   "outputs": [],
   "source": [
    "bright = cv2.imread('cube1.jpg')\n",
    "dark = cv2.imread('cube8.jpg')\n",
    "\n",
    "#python\n",
    "brightLAB = cv2.cvtColor(bright, cv2.COLOR_BGR2LAB)\n",
    "darkLAB = cv2.cvtColor(dark, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "brightYCB = cv2.cvtColor(bright, cv2.COLOR_BGR2YCrCb)\n",
    "darkYCB = cv2.cvtColor(dark, cv2.COLOR_BGR2YCrCb)\n",
    "\n",
    "brightHSV = cv2.cvtColor(bright, cv2.COLOR_BGR2HSV)\n",
    "darkHSV = cv2.cvtColor(dark, cv2.COLOR_BGR2HSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08d4a78",
   "metadata": {},
   "source": [
    "### Image Filtering Using Convolution in OpenCV\n",
    "\n",
    "```\n",
    "An Introduction to Convolution Kernels in Image Processing\n",
    "How to Use Kernels to Sharpen or Blur Images?\n",
    "Applying Identity Kernel to an Image in OpenCV\n",
    "Blurring an Image using a Custom 2D Convolution Kernel\n",
    "Blurring an image using OpenCV‚Äôs Built-In Blurring Function\n",
    "Applying Gaussian Blurring to an Image in OpenCV\n",
    "Applying Median Blurring to an Image in OpenCV\n",
    "Sharpening an Image using Custom 2D Convolution Kernel\n",
    "Applying Bilateral Filtering to an Image in OpenCV\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83aca53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "import cv2\n",
    "import numpy as np\n",
    " \n",
    "image = cv2.imread('test.jpg')\n",
    " \n",
    "# Print error message if image is null\n",
    "if image is None:\n",
    "    print('Could not read image')\n",
    " \n",
    "# Apply identity kernel\n",
    "kernel1 = np.array([[0, 0, 0],\n",
    "                    [0, 1, 0],\n",
    "                    [0, 0, 0]])\n",
    " \n",
    "identity = cv2.filter2D(src=image, ddepth=-1, kernel=kernel1)\n",
    " \n",
    "cv2.imshow('Original', image)\n",
    "cv2.imshow('Identity', identity)\n",
    "     \n",
    "cv2.waitKey()\n",
    "cv2.imwrite('identity.jpg', identity)\n",
    "cv2.destroyAllWindows()\n",
    " \n",
    "# Apply blurring kernel\n",
    "kernel2 = np.ones((5, 5), np.float32) / 25\n",
    "img = cv2.filter2D(src=image, ddepth=-1, kernel=kernel2)\n",
    " \n",
    "cv2.imshow('Original', image)\n",
    "cv2.imshow('Kernel Blur', img)\n",
    "     \n",
    "cv2.waitKey()\n",
    "cv2.imwrite('blur_kernel.jpg', img)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f8b892",
   "metadata": {},
   "source": [
    "**Blurring an Image using a Custom 2D-Convolution Kernel**\n",
    "\n",
    "Next, we will demonstrate how to blur an image. Here too, we will define a custom kernel, and use the filter2D() function in OpenCV to apply the filtering operation on the source image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebeba9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Apply blurring kernel\n",
    "\"\"\"\n",
    "kernel2 = np.ones((5, 5), np.float32) / 25\n",
    "img = cv2.filter2D(src=image, ddepth=-1, kernel=kernel2)\n",
    " \n",
    "cv2.imshow('Original', image)\n",
    "cv2.imshow('Kernel Blur', img)\n",
    " \n",
    "cv2.waitKey()\n",
    "cv2.imwrite('blur_kernel.jpg', img)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c5145a",
   "metadata": {},
   "source": [
    "**Blurring an Image Using OpenCV‚Äôs Built-In Function**\n",
    "\n",
    "You can also blur an image, using OpenCV‚Äôs built-in blur() function. Essentially a convenience function, use it to blur images, where you need not specifically define a kernel.  Simply specify the kernel size, using the ksize input argument, as shown in the code below. The blur function will then internally create a 5√ó5 blur kernel, and apply it to the source image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0f6e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Apply blur using `blur()` function\n",
    "\"\"\"\n",
    "img_blur = cv2.blur(src=image, ksize=(5,5)) # Using the blur function to blur an image where ksize is the kernel size\n",
    " \n",
    "# Display using cv2.imshow()\n",
    "cv2.imshow('Original', image)\n",
    "cv2.imshow('Blurred', img_blur)\n",
    " \n",
    "cv2.waitKey()\n",
    "cv2.imwrite('blur.jpg', img_blur)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325c23ec",
   "metadata": {},
   "source": [
    "**Applying Gaussian Blurring to an Image in OpenCV**\n",
    "\n",
    "We will now apply a Gaussian blur to an image, using OpenCV. This technique uses a Gaussian filter, which performs a weighted average, as opposed to the uniform average described in the first example. In this case, the Gaussian blur weights pixel values, based on their distance from the center of the kernel. Pixels further from the center have less influence on the weighted average. The following code convolves an image, using the GaussianBlur() function in OpenCV.\n",
    "\n",
    "```\n",
    "GaussianBlur(src, ksize, sigmaX[, dst[, sigmaY[, borderType]]])\n",
    "\n",
    "The GaussianBlur() function requires four input arguments:\n",
    "\n",
    "The first argument, src, specifies the source image that you want to filter.\n",
    "The second argument is ksize, which defines the size of the Gaussian kernel. Here, we are using a 5√ó5 kernel.\n",
    "The final two arguments are sigmaX and sigmaY, which are both set to 0. These are the Gaussian kernel standard deviations, in the X (horizontal) and Y (vertical) direction. The default setting of sigmaY is zero. If you simply  set sigmaX to zero, then the standard deviations are computed from the kernel size (width and height respectively). You can also explicitly set the size of each argument to positive values greater than zero.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f802fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Apply Gaussian blur\n",
    "\"\"\"\n",
    "# sigmaX is Gaussian Kernel standard deviation\n",
    "# ksize is kernel size\n",
    "gaussian_blur = cv2.GaussianBlur(src=image, ksize=(5,5), sigmaX=0, sigmaY=0)\n",
    " \n",
    "cv2.imshow('Original', image)\n",
    "cv2.imshow('Gaussian Blurred', gaussian_blur)\n",
    "     \n",
    "cv2.waitKey()\n",
    "cv2.imwrite('gaussian_blur.jpg', gaussian_blur)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab8674e",
   "metadata": {},
   "source": [
    "**Applying Median Blurring to an Image in OpenCV**\n",
    "\n",
    "We can also apply median blurring, using the medianBlur() function in OpenCV. In median blurring, each pixel in the source image is replaced by the median value of the image pixels in the kernel area.\n",
    "\n",
    "medianBlur(src, ksize)\n",
    "\n",
    "This function has just two required arguments:\n",
    "\n",
    "The first is the source image.\n",
    "The second is the kernel size, which must be an odd, positive integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c5a147",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Apply Median blur\n",
    "\"\"\"\n",
    "# medianBlur() is used to apply Median blur to image\n",
    "# ksize is the kernel size\n",
    "median = cv2.medianBlur(src=image, ksize=5)\n",
    " \n",
    "cv2.imshow('Original', image)\n",
    "cv2.imshow('Median Blurred', median)\n",
    "     \n",
    "cv2.waitKey()\n",
    "cv2.imwrite('median_blur.jpg', median)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893edca9",
   "metadata": {},
   "source": [
    "**Sharpening an Image Using Custom 2D-Convolution Kernels**\n",
    "\n",
    "You can also sharpen an image with a 2D-convolution kernel. First define a custom 2D kernel, and then use the filter2D() function to apply the convolution operation to the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90ba061",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Apply sharpening using kernel\n",
    "\"\"\"\n",
    "kernel3 = np.array([[0, -1,  0],\n",
    "                   [-1,  5, -1],\n",
    "                    [0, -1,  0]])\n",
    "sharp_img = cv2.filter2D(src=image, ddepth=-1, kernel=kernel3)\n",
    " \n",
    "cv2.imshow('Original', image)\n",
    "cv2.imshow('Sharpened', sharp_img)\n",
    "     \n",
    "cv2.waitKey()\n",
    "cv2.imwrite('sharp_image.jpg', sharp_img)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5ef5b6",
   "metadata": {},
   "source": [
    "**Applying Bilateral Filtering to an Image in OpenCV**\n",
    "```\n",
    "    bilateralFilter(src, d, sigmaColor, sigmaSpace)\n",
    "\n",
    "This function has four required arguments:\n",
    "\n",
    "The first argument of the function is the source image.\n",
    "The next argument d, defines the diameter of the pixel neighborhood used for filtering.\n",
    "The next two arguments, sigmaColor and sigmaSpace define the standard deviation of the (1D) color-intensity distribution and (2D) spatial distribution respectively.\n",
    "The sigmaSpace parameter defines the spatial extent of the kernel, in both the x and y directions (just like the Gaussian blur filter previously described).\n",
    "The sigmaColor parameter defines the one-dimensional Gaussian distribution, which specifies the degree to which differences in pixel intensity can be tolerated.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ebfd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Apply Bilateral Filtering\n",
    "\"\"\"\n",
    "# Using the function bilateralFilter() where d is diameter of each...\n",
    "# ...pixel neighborhood that is used during filtering.\n",
    "# sigmaColor is used to filter sigma in the color space.\n",
    "# sigmaSpace is used to filter sigma in the coordinate space.\n",
    "bilateral_filter = cv2.bilateralFilter(src=image, d=9, sigmaColor=75, sigmaSpace=75)\n",
    " \n",
    "cv2.imshow('Original', image)\n",
    "cv2.imshow('Bilateral Filtering', bilateral_filter)\n",
    " \n",
    "cv2.waitKey(0)\n",
    "cv2.imwrite('bilateral_filtering.jpg', bilateral_filter)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e573fd",
   "metadata": {},
   "source": [
    "### Image Thresholding in OpenCV\n",
    "\n",
    "**Global Thresholding**\n",
    "So, what is ‚Äòglobal‚Äô thresholding? When the thresholding rule is applied equally to every pixel in the image, and the threshold value is fixed, the operations are called global. \n",
    "\n",
    "Global thresholding algorithms take a source image (src) and a threshold value (thresh) as input, and produce an output image (dst), by comparing the pixel intensity at source pixel location (x,y) to the threshold. If src(x,y) > thresh, then dst(x,y) is assigned some value. Otherwise, dst(x,y) is assigned some other value.\n",
    "\n",
    "1. Binary Thresholding ( THRESH_BINARY )\n",
    "\n",
    "    The simplest form of global thresholding is called Binary Thresholding. \n",
    "\n",
    "    In addition to the source image (src) and the threshold value (thresh ), it takes another input parameter called the maximum value (maxValue).\n",
    "    At each pixel location (x,y), the pixel intensity at that location is compared to  a threshold value,  thresh .\n",
    "    If src(x,y) is greater than thresh, the thresholding operation sets the value of the destination image pixel dst(x,y) to the maxValue. Otherwise, it sets it to 0\n",
    "\n",
    "2. Inverse-Binary Thresholding ( THRESH_BINARY_INV )\n",
    "\n",
    "    zero, if the corresponding source pixel is greater than the threshold\n",
    "    maxValue, if the source pixel is less than the threshold\n",
    "\n",
    "3. Truncate Thresholding ( THRESH_TRUNC )\n",
    "    \n",
    "    The destination pixel is set to the threshold (thresh), if the source pixel value is greater than the threshold.\n",
    "    Otherwise, it is set to the source pixel value\n",
    "    The maxValue is ignored\n",
    "\n",
    "\n",
    "4. Threshold to Zero ( THRESH_TOZERO )\n",
    " \n",
    "    The destination pixel value is set to the pixel value of the corresponding source , if the source pixel value is greater than the threshold.\n",
    "    Otherwise, it is set to zero\n",
    "    The maxValue is ignored\n",
    "\n",
    "\n",
    "5. Inverted Threshold to Zero ( THRESH_TOZERO_INV )\n",
    "\n",
    "    The destination pixel value is set to zero, if the source pixel value is greater than the threshold.\n",
    "    Otherwise, it is set to the source pixel value\n",
    "    The  maxValue is ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e211c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "# import opencv\n",
    "import cv2\n",
    " \n",
    "# Read image\n",
    "src = cv2.imread(\"threshold.png\", cv2.IMREAD_GRAYSCALE)\n",
    " \n",
    "# Basic threhold example\n",
    "th, dst = cv2.threshold(src, 0, 255, cv2.THRESH_BINARY)\n",
    "cv2.imwrite(\"opencv-threshold-example.jpg\", dst)\n",
    " \n",
    "# Thresholding with maxValue set to 128\n",
    "th, dst = cv2.threshold(src, 0, 128, cv2.THRESH_BINARY)\n",
    "cv2.imwrite(\"opencv-thresh-binary-maxval.jpg\", dst)\n",
    " \n",
    "# Thresholding with threshold value set 127\n",
    "th, dst = cv2.threshold(src,127,255, cv2.THRESH_BINARY)\n",
    "cv2.imwrite(\"opencv-thresh-binary.jpg\", dst)\n",
    " \n",
    "# Thresholding using THRESH_BINARY_INV\n",
    "th, dst = cv2.threshold(src,127,255, cv2.THRESH_BINARY_INV)\n",
    "cv2.imwrite(\"opencv-thresh-binary-inv.jpg\", dst)\n",
    " \n",
    "# Thresholding using THRESH_TRUNC\n",
    "th, dst = cv2.threshold(src,127,255, cv2.THRESH_TRUNC)\n",
    "cv2.imwrite(\"opencv-thresh-trunc.jpg\", dst)\n",
    " \n",
    "# Thresholding using THRESH_TOZERO\n",
    "th, dst = cv2.threshold(src,127,255, cv2.THRESH_TOZERO)\n",
    "cv2.imwrite(\"opencv-thresh-tozero.jpg\", dst)\n",
    " \n",
    "# Thresholding using THRESH_TOZERO_INV\n",
    "th, dst = cv2.threshold(src,127,255, cv2.THRESH_TOZERO_INV)\n",
    "cv2.imwrite(\"opencv-thresh-to-zero-inv.jpg\", dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d0acbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Inverse Binary Threshold\n",
    "if src(x,y) > thresh:\n",
    "  dst(x,y) = 0\n",
    "else:\n",
    "  dst(x,y) = maxValue\n",
    "\n",
    "th, dst = cv2.threshold(src, thresh, maxValue, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "\n",
    "# Truncate Threshold\n",
    "if src(x,y) > thresh:\n",
    "  dst(x,y) = thresh\n",
    "else:\n",
    "  dst(x,y) = src(x,y)\n",
    "\n",
    "th, dst = cv2.threshold(src, thresh, maxValue, cv2.THRESH_TRUNC)\n",
    "\n",
    "\n",
    "# Threshold to Zero\n",
    "if src(x,y) > thresh:\n",
    "  dst(x,y) = src(x,y)\n",
    "else:\n",
    "  dst(x,y) = 0\n",
    "\n",
    "if src(x,y) > thresh:\n",
    "  dst(x,y) = 0\n",
    "else:\n",
    "  dst(x,y) = src(x,y)\n",
    "\n",
    "th, dst = cv2.threshold(src, thresh, maxValue, cv2.THRESH_TOZERO_INV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a163fac",
   "metadata": {},
   "source": [
    "**Adaptive Thresholding**\n",
    "\n",
    "Global thresholding can fail when an image has varying lighting conditions. Adaptive thresholding solves this by determining the threshold for each pixel based on a small surrounding region \n",
    "1.\n",
    "\n",
    "Two adaptive methods are available:\n",
    "\n",
    "ADAPTIVE_THRESH_MEAN_C ‚Äî threshold = mean of neighbourhood minus constant C\n",
    "ADAPTIVE_THRESH_GAUSSIAN_C ‚Äî threshold = Gaussian-weighted sum of neighbourhood minus constant C \n",
    "```python\n",
    "th = cv.adaptiveThreshold(img, 255, cv.ADAPTIVE_THRESH_GAUSSIAN_C, cv.THRESH_BINARY, 11, 2)\n",
    "```\n",
    "\n",
    "**Otsu's Binarization**\n",
    "\n",
    "Otsu's method automatically determines the optimal threshold from the image histogram, making it ideal for bimodal images (images with two distinct intensity peaks). It minimizes the weighted within-class variance \n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "# Apply Gaussian blur first to reduce noise, then Otsu's thresholding\n",
    "blur = cv.GaussianBlur(img, (5, 5), 0)\n",
    "ret, th = cv.threshold(blur, 0, 255, cv.THRESH_BINARY + cv.THRESH_OTSU)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8395f57d",
   "metadata": {},
   "source": [
    "### Blob Detection Using OpenCV\n",
    "\n",
    "**What is a Blob?**\n",
    "\n",
    "A Blob is a group of connected pixels in an image that share some common property ( E.g, grayscale value ). In the image above, the dark connected regions are blobs, and blob detection aims to identify and mark these regions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7948d202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimpleBlobDetector\n",
    "# Standard imports\n",
    "import cv2\n",
    "import numpy as np;\n",
    " \n",
    "# Read image\n",
    "im = cv2.imread(\"blob.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    " \n",
    "# Set up the detector with default parameters.\n",
    "detector = cv2.SimpleBlobDetector()\n",
    " \n",
    "# Detect blobs.\n",
    "keypoints = detector.detect(im)\n",
    " \n",
    "# Draw detected blobs as red circles.\n",
    "# cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS ensures the size of the circle corresponds to the size of blob\n",
    "im_with_keypoints = cv2.drawKeypoints(im, keypoints, np.array([]), (0,0,255), cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    " \n",
    "# Show keypoints\n",
    "cv2.imshow(\"Keypoints\", im_with_keypoints)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819c649b",
   "metadata": {},
   "source": [
    "**How Does Blob Detection Work?**\n",
    "\n",
    "SimpleBlobDetector, as the name implies, is based on a rather simple algorithm described below. The algorithm is controlled by parameters ( shown in bold below )  and has the following steps. Scroll down to learn how the parameters are set.\n",
    "```\n",
    "Thresholding : Convert the source images to several binary images by thresholding the source image with thresholds starting at minThreshold. These thresholds are incremented  by thresholdStep until maxThreshold. So the first threshold is minThreshold, the second is minThreshold + thresholdStep, the third is minThreshold + 2 x thresholdStep, and so on.\n",
    "Grouping : In each binary image,  connected white pixels are grouped.  Let‚Äôs call these binary blobs.\n",
    "Merging  : The centers of the binary blobs in the binary images are computed, and blobs located closer than minDistBetweenBlobs are merged.\n",
    "Center & Radius Calculation:  The centers and radii of the newly merged blobs are computed and returned.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baa019e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to set SimpleBlobDetector params?\n",
    "\t\n",
    "# Setup SimpleBlobDetector parameters.\n",
    "params = cv2.SimpleBlobDetector_Params()\n",
    " \n",
    "# Change thresholds\n",
    "params.minThreshold = 10\n",
    "params.maxThreshold = 200\n",
    " \n",
    "# Filter by Area.\n",
    "params.filterByArea = True\n",
    "params.minArea = 1500\n",
    " \n",
    "# Filter by Circularity\n",
    "params.filterByCircularity = True\n",
    "params.minCircularity = 0.1\n",
    " \n",
    "# Filter by Convexity\n",
    "params.filterByConvexity = True\n",
    "params.minConvexity = 0.87\n",
    " \n",
    "# Filter by Inertia\n",
    "params.filterByInertia = True\n",
    "params.minInertiaRatio = 0.01\n",
    " \n",
    "# Create a detector with the parameters\n",
    "ver = (cv2.__version__).split('.')\n",
    "if int(ver[0]) < 3 :\n",
    "  detector = cv2.SimpleBlobDetector(params)\n",
    "else :\n",
    "  detector = cv2.SimpleBlobDetector_create(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa1802c",
   "metadata": {},
   "source": [
    "### Edge Detection Using OpenCV\n",
    "\n",
    "1. How are Edges Detected?\n",
    "\n",
    "Sudden changes in pixel intensity characterize edges. We need to look for such changes in the neighboring pixels to detect edges. Let‚Äôs explore using two important edge-detection algorithms available in OpenCV: Sobel Edge Detection and Canny Edge Detection. We will discuss the theory as well as demonstrate the use of each in OpenCV.\n",
    "\n",
    "2. Sobel Edge Detection\n",
    "\n",
    "Sobel Edge Detection is one of the most widely used algorithms for edge detection. The Sobel Operator detects edges marked by sudden changes in pixel intensity.\n",
    "\n",
    "3. Canny Edge Detection\n",
    "\n",
    "Canny Edge Detection, the gradient magnitudes are compared with two threshold values, one smaller than the other. \n",
    "```\n",
    "If the gradient magnitude value is higher than the larger threshold value, those pixels are associated with solid edges and are included in the final edge map.\n",
    "If the gradient magnitude values are lower than the smaller threshold value, the pixels are suppressed and excluded from the final edge map.\n",
    "All the other pixels, whose gradient magnitudes fall between these two thresholds, are marked as ‚Äòweak‚Äô edges (i.e. they become candidates for being included in the final edge map). \n",
    "If the ‚Äòweak‚Äô pixels are connected to those associated with solid edges, they are also included in the final edge map. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8357f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "import cv2\n",
    " \n",
    "# Read the original image\n",
    "img = cv2.imread('test.jpg')\n",
    "# Display original image\n",
    "cv2.imshow('Original', img)\n",
    "cv2.waitKey(0)\n",
    " \n",
    "# Convert to graycsale\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "# Blur the image for better edge detection\n",
    "img_blur = cv2.GaussianBlur(img_gray, (3,3), 0)\n",
    " \n",
    "# Sobel Edge Detection\n",
    "sobelx = cv2.Sobel(src=img_blur, ddepth=cv2.CV_64F, dx=1, dy=0, ksize=5) # Sobel Edge Detection on the X axis\n",
    "sobely = cv2.Sobel(src=img_blur, ddepth=cv2.CV_64F, dx=0, dy=1, ksize=5) # Sobel Edge Detection on the Y axis\n",
    "sobelxy = cv2.Sobel(src=img_blur, ddepth=cv2.CV_64F, dx=1, dy=1, ksize=5) # Combined X and Y Sobel Edge Detection\n",
    "# Display Sobel Edge Detection Images\n",
    "cv2.imshow('Sobel X', sobelx)\n",
    "cv2.waitKey(0)\n",
    "cv2.imshow('Sobel Y', sobely)\n",
    "cv2.waitKey(0)\n",
    "cv2.imshow('Sobel X Y using Sobel() function', sobelxy)\n",
    "cv2.waitKey(0)\n",
    " \n",
    "# Canny Edge Detection\n",
    "edges = cv2.Canny(image=img_blur, threshold1=100, threshold2=200) # Canny Edge Detection\n",
    "# Display Canny Edge Detection Image\n",
    "cv2.imshow('Canny Edge Detection', edges)\n",
    "cv2.waitKey(0)\n",
    " \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff3cae6",
   "metadata": {},
   "source": [
    "### Mouse and Trackbar in OpenCV GUI\n",
    "\n",
    "    1. Annotating Images Using the Mouse\n",
    "    2. Resizing Images Using the Trackbar\n",
    "\n",
    "**Annotating Images Using the Mouse**\n",
    "\n",
    "OpenCV provides a mouse event-detection feature to detect various mouse operations like left-click and right-click. In this first example, we will show you how to use the mouse to render a rectangle, on an image displayed in a named window. \n",
    "```\n",
    "cv2.setMouseCallback(winname, onMouse, userdata)\n",
    "\n",
    " Arguments:\n",
    "    winname: Name of the window\n",
    "    onMouse: Callback function for mouse events\n",
    "    userdata: Optional argument passed to the callback\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825abbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "# Import packages\n",
    "import cv2\n",
    " \n",
    "# Lists to store the bounding box coordinates\n",
    "top_left_corner=[]\n",
    "bottom_right_corner=[]\n",
    " \n",
    "# function which will be called on mouse input\n",
    "def drawRectangle(action, x, y, flags, *userdata):\n",
    "  # Referencing global variables\n",
    "  global top_left_corner, bottom_right_corner\n",
    "  # Mark the top left corner when left mouse button is pressed\n",
    "  if action == cv2.EVENT_LBUTTONDOWN:\n",
    "    top_left_corner = [(x,y)]\n",
    "    # When left mouse button is released, mark bottom right corner\n",
    "  elif action == cv2.EVENT_LBUTTONUP:\n",
    "    bottom_right_corner = [(x,y)]   \n",
    "    # Draw the rectangle\n",
    "    cv2.rectangle(image, top_left_corner[0], bottom_right_corner[0], (0,255,0),2, 8)\n",
    "    cv2.imshow(\"Window\",image)\n",
    " \n",
    "# Read Images\n",
    "image = cv2.imread(\"../Input/sample.jpg\")\n",
    "# Make a temporary image, will be useful to clear the drawing\n",
    "temp = image.copy()\n",
    "# Create a named window\n",
    "cv2.namedWindow(\"Window\")\n",
    "# highgui function called when mouse events occur\n",
    "cv2.setMouseCallback(\"Window\", drawRectangle)\n",
    " \n",
    "k=0\n",
    "# Close the window when key q is pressed\n",
    "while k!=113:\n",
    "  # Display the image\n",
    "  cv2.imshow(\"Window\", image)\n",
    "  k = cv2.waitKey(0)\n",
    "  # If c is pressed, clear the window, using the dummy image\n",
    "  if (k == 99):\n",
    "    image= temp.copy()\n",
    "    cv2.imshow(\"Window\", image)\n",
    " \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec992ffa",
   "metadata": {},
   "source": [
    "**Resizing an Image Using the Trackbar**\n",
    "```\n",
    "cv2.createTrackbar( trackbarName, windowName, value, count, onChange)\n",
    "\n",
    "Arguments\n",
    "\n",
    "    trackbarname: Name of the created trackbar.\n",
    "    winname: Name of the parent window of the created trackbar.\n",
    "    value: Default position of the slider. This is optional.\n",
    "    count: Till what value the slider will go.\n",
    "    onChange: Callback function.\n",
    "    userdata: User data that is passed to the callback function. It can be used to handle trackbar events, without using global variables.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2debe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "# Import dependancies\n",
    "import cv2\n",
    " \n",
    "maxScaleUp = 100\n",
    "scaleFactor = 1\n",
    "windowName = \"Resize Image\"\n",
    "trackbarValue = \"Scale\"\n",
    " \n",
    "# read the image\n",
    "image = cv2.imread(\"../Input/sample.jpg\")\n",
    " \n",
    "# Create a window to display results and  set the flag to Autosize\n",
    "cv2.namedWindow(windowName, cv2.WINDOW_AUTOSIZE)\n",
    " \n",
    "# Callback functions\n",
    "def scaleImage(*args):\n",
    "    # Get the scale factor from the trackbar\n",
    "    scaleFactor = 1+ args[0]/100.0\n",
    "    # Resize the image\n",
    "    scaledImage = cv2.resize(image, None, fx=scaleFactor, fy = scaleFactor, interpolation = cv2.INTER_LINEAR)\n",
    "    cv2.imshow(windowName, scaledImage)\n",
    " \n",
    "# Create trackbar and associate a callback function\n",
    "cv2.createTrackbar(trackbarValue, windowName, scaleFactor, maxScaleUp, scaleImage)\n",
    " \n",
    "# Display the image\n",
    "cv2.imshow(windowName, image)\n",
    "c = cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c487c79",
   "metadata": {},
   "source": [
    "### Contour Detection using OpenCV\n",
    "\n",
    "**Application of Contours in Computer Vision**\n",
    " 1. Motion Detection\n",
    " 2. Unattended object detection\n",
    " 3. Background / Foreground Segmentation\n",
    "\n",
    "What are Contours\n",
    "When we join all the points on the boundary of an object, we get a contour. Typically, a specific contour refers to boundary pixels that have the same color and intensity. OpenCV makes it really easy to find and draw contours in images. It provides two simple functions:\n",
    "```\n",
    "    findContours()\n",
    "    drawContours()\n",
    "\n",
    "    Also, it has two different algorithms for contour detection:\n",
    "\n",
    "    CHAIN_APPROX_SIMPLE\n",
    "    CHAIN_APPROX_NONE\n",
    "```\n",
    "\n",
    "**Steps for Detecting and Drawing Contours in OpenCV**\n",
    "\n",
    "OpenCV makes this a fairly simple task. Just follow these steps:\n",
    "\n",
    "1. Read the Image and convert it to Grayscale Format\n",
    "    Read the image and convert the image to grayscale format.\n",
    "\n",
    "2. Apply Binary Thresholding\n",
    "    While finding contours, first always apply binary thresholding or Canny edge detection to the grayscale image. Here, we will apply binary thresholding.\n",
    "\n",
    "3. Find the Contours\n",
    "    Use the findContours() function to detect the contours in the image.\n",
    "\n",
    "4. Draw Contours on the Original RGB Image.\n",
    "    Once contours have been identified, use the drawContours() function to overlay the contours on the original RGB image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7c9f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding and Drawing Contours using OpenCV\n",
    "\n",
    "import cv2\n",
    " \n",
    "# read the image\n",
    "image = cv2.imread('input/image_1.jpg')\n",
    "\n",
    "# convert the image to grayscale format\n",
    "img_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# apply binary thresholding\n",
    "ret, thresh = cv2.threshold(img_gray, 150, 255, cv2.THRESH_BINARY)\n",
    "# visualize the binary image\n",
    "cv2.imshow('Binary image', thresh)\n",
    "cv2.waitKey(0)\n",
    "cv2.imwrite('image_thres1.jpg', thresh)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a40507",
   "metadata": {},
   "source": [
    "**Drawing Contours using CHAIN_APPROX_NONE**\n",
    "\n",
    "Now, let‚Äôs find and draw the contours, using the CHAIN_APPROX_NONE method. \n",
    "\n",
    "Start with the findContours() function. It has three required arguments, as shown below. For optional arguments, please refer to the documentation page here.\n",
    "```\n",
    "image: The binary input image obtained in the previous step.\n",
    "mode: This is the contour-retrieval mode. We provided this as RETR_TREE, which means the algorithm will retrieve all possible contours from the binary image. More contour retrieval modes are available, we will be discussing them too. You can learn more details on these options here. \n",
    "method: This defines the contour-approximation method. In this example, we will use CHAIN_APPROX_NONE\n",
    "```\n",
    "\n",
    "Next, use the drawContours() function to overlay the contours on the RGB image. This function has four required and several optional arguments. The first four arguments below are required. For the optional arguments, please refer to the documentation page here.\n",
    "```\n",
    "image: This is the input RGB image on which you want to draw the contour.\n",
    "contours: Indicates the contours obtained from the findContours() function.\n",
    "contourIdx: The pixel coordinates of the contour points are listed in the obtained contours. Using this argument, you can specify the index position from this list, indicating exactly which contour point you want to draw. Providing a negative value will draw all the contour points.\n",
    "color: This indicates the color of the contour points you want to draw. We are drawing the points in green.\n",
    "thickness: This is the thickness of contour points.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714bbae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing Contours using CHAIN_APPROX_NONE\n",
    "\n",
    "# detect the contours on the binary image using cv2.CHAIN_APPROX_NONE\n",
    "contours, hierarchy = cv2.findContours(image=thresh, mode=cv2.RETR_TREE, method=cv2.CHAIN_APPROX_NONE)\n",
    "                                      \n",
    "# draw contours on the original image\n",
    "image_copy = image.copy()\n",
    "cv2.drawContours(image=image_copy, contours=contours, contourIdx=-1, color=(0, 255, 0), thickness=2, lineType=cv2.LINE_AA)\n",
    "                \n",
    "# see the results\n",
    "cv2.imshow('None approximation', image_copy)\n",
    "cv2.waitKey(0)\n",
    "cv2.imwrite('contours_none_image1.jpg', image_copy)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484f76ae",
   "metadata": {},
   "source": [
    "**Drawing Contours using CHAIN_APPROX_SIMPLE**\n",
    "\n",
    "Let‚Äôs find out now how the CHAIN_APPROX_SIMPLE algorithm works and what makes it different from the CHAIN_APPROX_NONE algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a27efbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now let's try with `cv2.CHAIN_APPROX_SIMPLE`\n",
    "\"\"\"\n",
    "# detect the contours on the binary image using cv2.ChAIN_APPROX_SIMPLE\n",
    "contours1, hierarchy1 = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "# draw contours on the original image for `CHAIN_APPROX_SIMPLE`\n",
    "image_copy1 = image.copy()\n",
    "cv2.drawContours(image_copy1, contours1, -1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "# see the results\n",
    "cv2.imshow('Simple approximation', image_copy1)\n",
    "cv2.waitKey(0)\n",
    "cv2.imwrite('contours_simple_image1.jpg', image_copy1)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcc5175",
   "metadata": {},
   "source": [
    "The only difference here is that we specify the method for findContours() as CHAIN_APPROX_SIMPLE instead of CHAIN_APPROX_NONE.\n",
    "\n",
    "The CHAIN_APPROX_SIMPLE  algorithm compresses horizontal, vertical, and diagonal segments along the contour and leaves only their end points. This means that any of the points along the straight paths will be dismissed, and we will be left with only the end points. For example, consider a contour, along a rectangle. All the contour points, except the four corner points will be dismissed. This method is faster than the CHAIN_APPROX_NONE because the algorithm does not store all the points, uses less memory, and therefore, takes less time to execute.\n",
    "\n",
    "If you observe closely, there are almost no differences between the outputs of CHAIN_APPROX_NONE and CHAIN_APPROX_SIMPLE. \n",
    "\n",
    "Now, why is that?\n",
    "\n",
    "The credit goes to the drawContours() function. Although the CHAIN_APPROX_SIMPLE method typically results in fewer points, the drawContours() function automatically connects adjacent points, joining them even if they are not in the contours list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad33446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to actually visualize the effect of `CHAIN_APPROX_SIMPLE`, we need a proper image\n",
    "image1 = cv2.imread('input/image_2.jpg')\n",
    "img_gray1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    " \n",
    "ret, thresh1 = cv2.threshold(img_gray1, 150, 255, cv2.THRESH_BINARY)\n",
    "contours2, hierarchy2 = cv2.findContours(thresh1, cv2.RETR_TREE,\n",
    "                                               cv2.CHAIN_APPROX_SIMPLE)\n",
    "image_copy2 = image1.copy()\n",
    "cv2.drawContours(image_copy2, contours2, -1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "cv2.imshow('SIMPLE Approximation contours', image_copy2)\n",
    "cv2.waitKey(0)\n",
    "image_copy3 = image1.copy()\n",
    "for i, contour in enumerate(contours2): # loop over one contour area\n",
    "   for j, contour_point in enumerate(contour): # loop over the points\n",
    "       # draw a circle on the current contour coordinate\n",
    "       cv2.circle(image_copy3, ((contour_point[0][0], contour_point[0][1])), 2, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "# see the results\n",
    "cv2.imshow('CHAIN_APPROX_SIMPLE Point only', image_copy3)\n",
    "cv2.waitKey(0)\n",
    "cv2.imwrite('contour_point_simple.jpg', image_copy3)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d14ee84",
   "metadata": {},
   "source": [
    "### Simple Background Estimation in Videos using OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d580749f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the Median for Background Estimation\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage import data, filters\n",
    " \n",
    "# Open Video\n",
    "cap = cv2.VideoCapture('video.mp4')\n",
    " \n",
    "# Randomly select 25 frames\n",
    "frameIds = cap.get(cv2.CAP_PROP_FRAME_COUNT) * np.random.uniform(size=25)\n",
    " \n",
    "# Store selected frames in an array\n",
    "frames = []\n",
    "for fid in frameIds:\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, fid)\n",
    "    ret, frame = cap.read()\n",
    "    frames.append(frame)\n",
    " \n",
    "# Calculate the median along the time axis\n",
    "medianFrame = np.median(frames, axis=0).astype(dtype=np.uint8)   \n",
    " \n",
    "# Display median frame\n",
    "cv2.imshow('frame', medianFrame)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b6be12",
   "metadata": {},
   "source": [
    "**Frame differencing**\n",
    "\n",
    "The obvious next question is if we can create a mask for every frame which shows parts of the image that are in motion.\n",
    "\n",
    "This is accomplished in the following steps\n",
    "```\n",
    "    Convert the median frame to grayscale.\n",
    "    Loop over all frames in the video. Extract the current frame and convert it to grayscale.\n",
    "    Calcualte the absolute difference between the current frame and the median frame.\n",
    "    Threshold the above image to remove noise and binarize the output.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d82280b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset frame number to 0\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    " \n",
    "# Convert background to grayscale\n",
    "grayMedianFrame = cv2.cvtColor(medianFrame, cv2.COLOR_BGR2GRAY)\n",
    " \n",
    "# Loop over all frames\n",
    "ret = True\n",
    "while(ret):\n",
    " \n",
    "  # Read frame\n",
    "  ret, frame = cap.read()\n",
    "  # Convert current frame to grayscale\n",
    "  frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "  # Calculate absolute difference of current frame and\n",
    "  # the median frame\n",
    "  dframe = cv2.absdiff(frame, grayMedianFrame)\n",
    "  # Treshold to binarize\n",
    "  th, dframe = cv2.threshold(dframe, 30, 255, cv2.THRESH_BINARY)\n",
    "  # Display image\n",
    "  cv2.imshow('frame', dframe)\n",
    "  cv2.waitKey(20)\n",
    " \n",
    "# Release video object\n",
    "cap.release()\n",
    " \n",
    "# Destroy all windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a29c09",
   "metadata": {},
   "source": [
    "## Deep Learning with OpenCV DNN Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4219b32c",
   "metadata": {},
   "source": [
    "**What is OpenCV DNN Module?**\n",
    "\n",
    "We all know OpenCV as one of the best computer vision libraries. Additionally, it also has functionalities for running deep learning inference as well. The best part is supporting the loading of different models from different frameworks, using which we can carry out several deep learning functionalities. The feature of supporting models from different frameworks has been a part of OpenCV since version 3.3. Still, many newcomers to the field are unaware of this great feature of OpenCV. Therefore, they tend to miss many fun and good learning opportunities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0cde45",
   "metadata": {},
   "source": [
    "Different Models that OpenCV DNN Module Supports\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e-kyc-using-cn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
